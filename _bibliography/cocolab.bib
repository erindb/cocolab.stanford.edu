%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Noah Goodman at 2015-05-08 13:11:07 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Icard2015,
	Author = {Icard III, Thomas Frederick and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:44:29 +0000},
	Date-Modified = {2015-04-02 13:51:29 +0000},
	Title = {A Resource-Rational Approach to the Causal Frame Problem},
	Year = {To appear}}

@inproceedings{Sumner2015,
	Author = {Emily Sumner and Erika DeAngelis and Mara Hyatt and Noah D. Goodman and Celeste Kidd},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:42:04 +0000},
	Date-Modified = {2015-04-02 13:43:36 +0000},
	Title = {Toddlers Always Get the Last Word: Recency biases in early verbal behavior},
	Year = {To appear}}

@inproceedings{Hawthonre2015,
	Annote = {(to appear)},
	Author = {Daniel Hawthorne and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:40:58 +0000},
	Date-Modified = {2015-04-02 13:41:27 +0000},
	Title = {So good it has to be true: Wishful thinking in theory of mind},
	Year = {To appear}}

@inproceedings{Krafft2015,
	Author = {Peter M. Krafft and Robert X. D. Hawkins and Alex ``Sandy'' Pentland and Noah D. Goodman and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:29:39 +0000},
	Date-Modified = {2015-04-02 13:40:05 +0000},
	Title = {Emergent Collective Sensing in Human Groups},
	Year = {To appear}}

@inproceedings{Kao2015b,
	Annote = {(to appear)},
	Author = {Justine T Kao and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:28:21 +0000},
	Date-Modified = {2015-04-02 13:28:59 +0000},
	Title = {Let's talk (ironically) about the weather: Modeling verbal irony},
	Year = {To appear}}

@inproceedings{Hawkins2015,
	Annote = {(to appear)},
	Author = {Robert X. D. Hawkins and Andreas Stuhlm\"uller and Judith Degen and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:23:04 +0000},
	Date-Modified = {2015-04-02 13:24:53 +0000},
	Title = {Why do you ask? Good questions provoke informative answers},
	Year = {To appear}}

@inproceedings{Gweon2015,
	Annote = {(to appear)},
	Author = {Ilona Bass and Daniel Hawthorne and Noah D. Goodman and Hyowon Gweon},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:20:11 +0000},
	Date-Modified = {2015-04-02 13:22:10 +0000},
	Title = {Not by number alone: The effect of teacher's knowledge and its value in evaluating "sins of omission"},
	Year = {To appear}}

@inproceedings{Bennett2015,
	Annote = {(to appear)},
	Author = {Erin D. Bennett and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:18:20 +0000},
	Date-Modified = {2015-04-02 13:19:09 +0000},
	Title = {Extremely costly intensifiers are stronger than quite costly ones},
	Year = {To appear}}

@inproceedings{Ong2015,
	Annote = {(to appear)},
	Author = {Desmond Ong and Noah D. Goodman and Jamil Zaki},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:16:33 +0000},
	Date-Modified = {2015-04-02 13:18:01 +0000},
	Title = {Near-misses sting even when they are uncontrollable},
	Year = {To appear}}

@inproceedings{Gerstenberg2015,
	Annote = {(to appear)},
	Author = {Gerstenberg, Tobias and Noah D. Goodman and Lagnado, David A and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:15:12 +0000},
	Date-Modified = {2015-04-02 13:16:22 +0000},
	Title = {How, whether, why: Causal judgments as counterfactual contrasts},
	Year = {To appear}}

@inproceedings{Degen2015,
	Annote = {(to appear)},
	Author = {Judith Degen and Michael Henry Tessler and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2015-04-02 13:11:36 +0000},
	Date-Modified = {2015-04-02 13:14:35 +0000},
	Website = {https://5ac76e91-a-62cb3a1a-s-sites.googlegroups.com/site/judithdegen/publications/DegenTesslerGoodman2015.pdf?attachauth=ANoY7cp5OmaeLAni1LLfF17gRP9DL8RV89Vnxrkui3qc1bpI88Ms0XPs7k8FGKy2kGxOSogJKGbnWojT6bDTZmEnQrqaDoQW94Vtie89IblXtkEspDHQY2CzdCtEBVLzsuO7Uqeayo_fxNWBUvntYEOZxido06kQJzBbbnSIeD00EaaRcW6rMjWg1wx_J3K_XOsBPWUInFx2lABMYR-uPsgXLFRgB-3EQlksADv4-jcdJ5l_xR2PvPlfqkgs82gU3J-2dBf-JLSl&attredirects=0},
	Title = {Wonky worlds: Listeners revise world knowledge when utterances are odd},
	Year = {To appear}}

@article{Kao2015,
	Abstract = {Humor plays an essential role in human interactions. Precisely what makes something funny, however, remains elusive. While research on natural language understanding has made significant advancements in recent years, there has been little direct integration of humor research with computational models of language understanding. In this paper, we propose two information-theoretic measures---ambiguity and distinctiveness---derived from a simple model of sentence processing. We test these measures on a set of puns and regular sentences and show that they correlate significantly with human judgments of funniness. Moreover, within a set of puns, the distinctiveness measure distinguishes exceptionally funny puns from mediocre ones. Our work is the first, to our knowledge, to integrate a model of general language understanding and humor theory to quantitatively predict humor at a fine-grained level. We present it as an example of a framework for applying models of language processing to understand higher-level linguistic and cognitive phenomena.},
	Annote = {(to appear)},
	Author = {Justine T Kao and Roger Levy and Noah D Goodman},
	Date-Added = {2015-04-01 14:54:52 +0000},
	Date-Modified = {2015-04-07 13:23:58 +0000},
	Journal = {Cognitive Science},
	Title = {A computational model of linguistic humor in puns},
	Website = {//cocolab.stanford.edu/papers/kaoetal2015-puns-pre.pdf},
	Year = {To appear}}

@inproceedings{GraphicsHMC,
	Abstract = {We present a system for generating suggestions from highly-constrained, continuous design spaces. We formulate suggestion as sampling from a probability distribution; constraints are represented as factors that concentrate probability mass around sub-manifolds of the design space. These sampling problems are intractable using typical random walk MCMC techniques, so we adopt Hamiltonian Monte Carlo (HMC), a gradient-based MCMC method. We implement HMC in a high-performance probabilistic programming language, and we evaluate its ability to efficiently generate suggestions for two different, highly-constrained example applications: vector art coloring and designing stable stacking structures.},
	Annote = {<b>[Best paper award honorable mention.]</b>},
	Author = {Daniel Ritchie and Sharon Lin and Noah D. Goodman and Pat Hanrahan},
	Booktitle = {Proceedings of Eurographics 2015},
	Date-Modified = {2015-05-08 20:10:36 +0000},
	Title = {{Generating Design Suggestions under Tight Constraints with Gradient-based Probabilistic Programming}},
	Website = {//stanford.edu/~dritchie/graphics-hmc.pdf},
	Year = {2015}}

@inproceedings{rictchie2015controlling,
	Annote = {(to appear)},
	Author = {Daniel Ritchie and B. Mildenhall and N. D. Goodman and P. Hanrahan},
	Booktitle = {SIGGRAPH 2015},
	Title = {Controlling Procedural Modeling Programs with Stochastically-Ordered Sequential Monte Carlo},
	Website = {//stanford.edu/~dritchie/procmod-smc.pdf},
	Year = {To appear}}

@article{lassiterAdjectivalVagueness,
	Annote = {(to appear)},
	Author = {D. Lassiter and N. D. Goodman},
	Journal = {Synthese},
	Title = {Adjectival vagueness in a Bayesian model of interpretation},
	Year = {To appear}}

@article{Pierson2014,
	Abstract = {Classical decision theory predicts that people should be indifferent to information that is not useful for making decisions, but this model often fails to describe human behavior. Here we investigate one such scenario, where people desire information about whether an event (the gain/loss of money) will occur even though there is no obvious decision to be made on the basis of this information. We find a curious dual trend: if information is costless, as the probability of the event increases people want the information more; if information is not costless, people's desire for the information peaks at an intermediate probability. People also want information more as the importance of the event increases, and less as the cost of the information increases. We propose a model that explains these results, based on the assumption that people have limited cognitive resources and obtain information about which events will occur so they can determine whether to expend effort planning for them.},
	Author = {E. Pierson and N. D. Goodman},
	Journal = {PLoS ONE},
	Title = {Uncertainty and denial: a resource-rational model of the value of information},
	Website = {//cocolab.stanford.edu/papers/Pierson2014},
	Year = {2014}}

@article{Lassiter2014,
	Abstract = {The "new paradigm" unifying deductive and inductive reasoning in a Bayesian framework (Oaksford & Chater, 2007; Over, 2009) has been claimed to be falsified by results which show sharp differences between reasoning about necessity vs. plausibility (Heit & Rotello, 2010; Rips, 2001; Rotello & Heit, 2009). We provide a probabilistic model of reasoning with modal expressions such as "necessary" and "plausible" informed by recent work in formal semantics of natural language, and show that it predicts the possibility of non-linear response patterns which have been claimed to be problematic. Our model also makes a strong monotonicity prediction, while two-dimensional theories predict the possibility of reversals in argument strength depending on the modal word chosen. Predictions were tested using a novel experimental paradigm that replicates the previously-reported response patterns with a minimal manipulation, changing only one word of the stimulus between conditions. We found a spectrum of reasoning "modes" corresponding to different modal words, and strong support for our model's monotonicity prediction. This indicates that probabilistic approaches to reasoning can account in a clear and parsimonious way for data previously argued to falsify them, as well as new, more fine-grained, data. It also illustrates the importance of careful attention to the semantics of language employed in reasoning experiments.},
	Author = {D. Lassiter and N. D. Goodman},
	Journal = {Cognition},
	Title = {How many kinds of reasoning? Inference, probability, and natural language semantics},
	Website = {//web.stanford.edu/~danlass/Lassiter-Goodman-reasoning.pdf},
	Year = {2015}}

@article{Bergen2014_2,
	Abstract = {We combine two recent probabilistic approaches to natural language understanding, exploring the formal pragmatics of communication on a noisy channel. We first extend a model of rational communication between a speaker and listener, to allow for the possibility that messages are corrupted by noise. In this model, common knowledge of a noisy channel leads to the use and correct understanding of sentence fragments. A further extension of the model, which allows the speaker to intentionally reduce the noise rate on a word, is used to model prosodic emphasis. We show that the model derives several well-known changes in meaning associated with prosodic emphasis. Our results show that nominal amounts of actual noise can be leveraged for communicative purposes.},
	Annote = {(to appear)},
	Author = {L. Bergen and N. D. Goodman},
	Journal = {Topics in Cognitive Science},
	Title = {The strategic use of noise in pragmatic reasoning},
	Website = {//cocolab.stanford.edu/papers/PragmaticProsody.pdf},
	Year = {To appear}}

@article{Goodman2014,
	Abstract = {Computational models in psychology are precise, fully explicit scientific hypotheses. Over the past 15 years, probabilistic modeling of human cognition has yielded quantitative theories of a wide variety of reasoning and learning phenomena. Recently, Marcus and Davis (2013) critique several examples of this work, using these critiques to question the basic validity of the probabilistic approach. Contra the broad rhetoric of their article, the points made by Marcus and Davis -- while useful to consider -- do not indicate systematic problems with the probabilistic modeling enterprise.},
	Author = {N. D. Goodman and M. C. Frank and T. L. Griffiths and J. B. Tenenbaum and P. Battaglia and J. Hamrick},
	Journal = {Psychological Science},
	Title = {Relevant and robust. A response to Marcus and Davis},
	Website = {//langcog.stanford.edu/papers/GFGTBH-psychsci.pdf},
	Year = {2015}}

@article{Kao2014hyperbole,
	Abstract = {One of the most puzzling and important facts about communication is that people do not always mean what they say; speakers often use imprecise, exaggerated, or otherwise literally false descriptions to communicate experiences and attitudes. Here, we focus on the nonliteral interpretation of number words, in particular hyperbole (interpreting unlikely numbers as exaggerated and conveying affect) and pragmatic halo (interpreting round numbers imprecisely). We provide a computational model of number interpretation as social inference regarding the communicative goal, meaning, and affective subtext of an utterance. We show that our model predicts humans' interpretation of number words with high accuracy. Our model is the first to our knowledge to incorporate principles of communication and empirically measured background knowledge to quantitatively predict hyperbolic and pragmatic halo effects in number interpretation. This modeling framework provides a unified approach to nonliteral language understanding more generally.},
	Author = {Justine T Kao and Jean Wu and Leon Bergen and Noah D Goodman},
	Date-Added = {2014-07-02 14:29:32 +0000},
	Date-Modified = {2014-08-26 20:27:55 +0000},
	Journal = {Proceedings of the {N}ational {A}cademy of {S}ciences},
	Title = {Nonliteral understanding of number words},
	Website = {//cocolab.stanford.edu/papers/PNAS-2014-KaoEtAl.pdf},
	Year = {2014}}

@article{StillerEtAl2014,
	Abstract = {If a speaker tells us that "some guests were late to the party," we typically infer that not all were. Implicatures, in which an ambiguous statement ("some and possibly all") is strengthened pragmatically (to "some and not all"), are a paradigm case of pragmatic reasoning. Inferences of this sort are difficult for young children, but recent work suggests that this mismatch may stem from issues in understanding the relationship between lexical items like "some" and "all," rather than broader pragmatic deficits. We tested children's ability to make non-quantificational pragmatic inferences by constructing contextually-derived "ad-hoc" implicatures, using sets of pictures with contrasting features. We found that four-year-olds and some three-year-olds were able to make implicatures successfully using these displays. Hence, apparent failures in scalar implicature are likely due to difficulties specific to the constructions and tasks used in previous work; these difficulties may have masked aspects of children's underlying pragmatic competence.},
	Author = {Stiller, A. J. and Goodman, N. D. and Frank, M. C.},
	Date-Modified = {2014-07-01 15:38:39 +0000},
	Journal = {Language Learning and Development},
	Title = {Ad-hoc scalar implicature in preschool children},
	Website = {//langcog.stanford.edu/papers/SGF-lldinpress.pdf},
	Year = {2014}}

@inproceedings{YangEtAl2014,
	Abstract = {Universal probabilistic programming languages (such as Church) trade performance for abstraction: any model can be represented compactly as an arbitrary stochastic computation, but costly online analyses are required for inference. We present a technique that recovers hand-coded levels of performance from a universal probabilistic language, for the Metropolis-Hastings (MH) MCMC inference algorithm. It takes a Church program as input and traces its execution to remove computation overhead. It then analyzes the trace for each proposal, using slicing, to identify the minimal computation needed to evaluate the MH acceptance probability. Generated incremental code is much faster than a baseline implementation (up to 600x) and usually as fast as handcoded MH kernels.},
	Author = {Yang, L. and Hanrahan, P., and Goodman, N. D.},
	Booktitle = {AISTATS},
	Title = {Generating Efficient MCMC Kernels from Probabilistic Programs},
	Website = {//www.stanford.edu/~ngoodman/papers/aistats2014-shred.pdf},
	Year = {2014}}

@article{ShaftoEtAl2014,
	Abstract = {Much of learning and reasoning occurs in pedagogical situations -- situations in which a person who knows a concept chooses examples for the purpose of helping a learner acquire the concept. We introduce a model of teaching and learning in pedagogical settings that predicts which examples teachers should choose and what learners should infer given a teacher's examples. We present three experiments testing the model predictions for rule-based, prototype, and causally structured concepts. The model shows good quantitative and qualitative fits to the data across all three experiments, predicting novel qualitative phenomena in each case. We conclude by discussing implications for understanding concept learning and implications for theoretical claims about the role of pedagogy in human learning.},
	Author = {Patrick Shafto and Noah D Goodman and Thomas L. Griffiths},
	Journal = {Cognitive Psychology},
	Title = {A rational account of pedagogical reasoning: Teaching by, and learning from, examples},
	Website = {//www.stanford.edu/~ngoodman/papers/shaftogg14.pdf},
	Year = {2014}}

@unpublished{BergenLevyGoodman,
	Abstract = {A number of recent proposals have used techniques from game theory and Bayesian cognitive science to formalize Gricean pragmatic reasoning (Frank & Goodman, 2012; Franke, 2009; Goodman & Stuhlm\"uller, 2013; Jaeger, 2012). We discuss two phenomena which pose a challenge to these accounts of pragmatics: M-implicatures (Horn, 1984) and embedded implicatures which violate Hurford's constraint (Chierchia, Fox & Spector, 2012; Hurford, 1974). Previous models cannot derive these implicatures, because of basic limitations in their architecture. In order to explain these phenomena, we propose a realignment of the division between semantic content and pragmatic content. Under this proposal, the semantic content of an utterance is not fixed independent of pragmatic inference; rather, pragmatic inference partially determines an utterance's semantic content. We show how semantic inference can be realized as an extension to the Rational Speech Acts framework (Goodman & Stuhlm\"uller, 2013). The addition of lexical uncertainty derives both M-implicatures and the relevant embedded implicatures, and preserves the derivations of more standard implicatures.},
	Annote = {(Unpublished manuscript.)},
	Author = {Leon Bergen and Roger Levy and Noah D Goodman},
	Date-Modified = {2014-07-02 14:41:59 +0000},
	Title = {Pragmatic Reasoning through Semantic Inference},
	Website = {//web.mit.edu/bergen/www/papers/BergenLevyGoodman2014.pdf},
	Year = {Manuscript}}

@inproceedings{Tessler:2014wu,
	Abstract = {Syllogistic reasoning lies at the intriguing intersection of natural and formal reasoning, of language and logic. Syllogisms comprise a formal system of reasoning yet use natural language quantifiers, and invite natural language conclusions. How can we make sense of the interplay between logic and language? We develop a computational-level theory that considers reasoning over concrete situations, constructed probabilistically by sampling. The base model can be enriched to consider the pragmatics of natural language arguments. The model predictions are compared with behavioral data from a recent meta-analysis. The flexibility of the model is then explored in a data set of syllogisms using the generalized quantifiers most and few. We conclude by relating our model to two extant theories of syllogistic reasoning -- Mental Models and Probability Heuristics.},
	Author = {Tessler, Michael Henry and Goodman, Noah D.},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-07 15:39:55 +0000},
	Date-Modified = {2014-04-07 15:40:29 +0000},
	Title = {Some arguments are probably valid: Syllogistic reasoning as communication},
	Website = {https://www.stanford.edu/~ngoodman/papers/cogsci14-syllogisms_tessler.pdf},
	Year = {2014}}

@inproceedings{Gerstenberg:2014qr,
	Abstract = {In this paper, we demonstrate that people's causal judgments are inextricably linked to counterfactuals. In our experiments, participants judge whether one billiard ball A caused another ball B to go through a gate. Our counterfactual simulation model predicts that people arrive at their causal judgments by comparing what actually happened with the result of mentally simulating what would have happened in the relevant counterfactual world. We test our model against actualist theories of causation which aim to explain causation just in terms of what actually happened. Our experimental stimuli contrast cases in which we hold constant what actually happened but vary the counterfactual outcome. In support of our model, we find that participants' causal judgments differ drastically between such cases. People's cause and prevention judgments increase with their subjective degree of belief that the counterfactual outcome would have been different from what actually happened.},
	Author = {Gerstenberg, Tobias and Noah D. Goodman and Lagnado, David A and Tenenbaum, Joshua B},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-07 15:38:57 +0000},
	Date-Modified = {2014-04-07 15:39:35 +0000},
	Title = {From counterfactual simulation to causal judgment},
	Website = {//cocolab.stanford.edu/papers/GerstenbergEtAl2014.pdf},
	Year = {2014}}

@inproceedings{Gershman:2014wt,
	Abstract = {Recent studies of probabilistic reasoning have postulated general-purpose inference algorithms that can be used to answer arbitrary queries. These algorithms are memoryless, in the sense that each query is processed independently, without reuse of earlier computation. We argue that the brain operates in the setting of amortized inference, where numerous related queries must be answered (e.g., recognizing a scene from multiple viewpoints); in this setting, memoryless algorithms can be computationally wasteful. We propose a simple form of flexible reuse, according to which shared inferences are cached and composed together to answer new queries. We present experimental evidence that humans exploit this form of reuse: the answer to a complex query can be systematically predicted from a person's response to a simpler query if the simpler query was presented first and entails a sub-inference (i.e., a sub-component of the more complex query). People are also faster at answering a complex query when it is preceded by a sub-inference. Our results suggest that the astonishing efficiency of human probabilistic reasoning may be supported by interactions between inference and memory.},
	Author = {Sam Gershman and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-07 15:38:04 +0000},
	Date-Modified = {2014-04-07 15:38:45 +0000},
	Title = {Amortized inference in probabilistic reasoning},
	Website = {//www.stanford.edu/~ngoodman/papers/amortized_inference.pdf},
	Year = {2014}}

@inproceedings{Degen:2014fu,
	Abstract = {A rarely discussed but important issue in research on pragmatic inference is the choice of dependent measure for estimating the robustness of pragmatic inferences and their sensitivity to contextual manipulations. Here we present the results from three studies exploring the effect of contextual manipulations on scalar implicature. In all three studies we manipulate the salient question under discussion and the perceptual availability of relevant set sizes. The studies differ only in the dependent measure used: Exp. 1 uses truth judgements, Exp. 2 uses word probability ratings, and Exp. 3 uses a direct measure of sentence interpretation. We argue that the first two are effectively measures of production, and find they are sensitive to our contextual manipulations. In contrast the interpretation measure shows no effect of context. We argue that this methodologically troubling finding can be understood and predicted by using the framework of probabilistic pragmatics.},
	Author = {Judith Degen and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-07 15:36:03 +0000},
	Date-Modified = {2014-04-07 15:37:27 +0000},
	Title = {Lost your marbles? The puzzle of dependent measures in experimental pragmatics},
	Website = {//cocolab.stanford.edu/papers/DegenGoodman2014.pdf},
	Year = {2014}}

@inproceedings{Bergen2014,
	Abstract = {We combine two recent probabilistic approaches to natural language understanding, exploring the formal pragmatics of communication on a noisy channel. We show that nominal amounts of actual noise can be leveraged for communicative purposes. Common knowledge of a noisy channel leads to the use and correct understanding of sentence fragments. Prosodic emphasis, interpreted as an intentional action to reduce noise on a word, results in strengthened meanings.},
	Annote = {<b>[Winner of the 2014 Cognitive Science Society computational modeling prize for Language.]</b>},
	Author = {Bergen, Leon and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-04 15:59:55 +0000},
	Date-Modified = {2014-04-07 15:34:52 +0000},
	Title = {The strategic use of noise in pragmatic reasoning},
	Website = {//web.mit.edu/bergen/www/papers/BergenGoodman2014.pdf},
	Year = {2014}}

@inproceedings{Kao2014,
	Abstract = {While the ubiquity and importance of nonliteral language are clear, people's ability to use and understand it remains a mystery. Metaphor in particular has been studied extensively across many disciplines in cognitive science. One approach focuses on the pragmatic principles that listeners utilize to infer meaning from metaphorical utterances. While this approach has generated a number of insights about how people understand metaphor, to our knowledge there is no formal model showing that effects in metaphor understanding can arise from basic principles of communication. Building upon recent advances in formal models of pragmatics, we describe a computational model that uses pragmatic reasoning to interpret metaphorical utterances. We conduct behavioral experiments to evaluate the model's performance and show that our model produces metaphorical interpretations that closely fit behavioral data. We discuss implications of the model for metaphor understanding, principles of communication, and formal models of language understanding},
	Author = {Kao, Justine T and Bergen, Leon and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-04 15:57:18 +0000},
	Date-Modified = {2014-04-07 15:34:44 +0000},
	Title = {Formalizing the pragmatics of metaphor understanding},
	Website = {//cocolab.stanford.edu/papers/KaoEtAl2014.pdf},
	Year = {2014}}

@book{dippl,
	Annote = {(<code>//dippl.org</code>)},
	Author = {Goodman, N. D. and Stuhlm\"uller, A.},
	Title = {The Design and Implementation of Probabilistic Programming Languages},
	Website = {//dippl.org}}

@book{probmods,
	Abstract = {In this book, we explore the probabilistic approach to cognitive science, which models learning and reasoning as inference in complex probabilistic models. In particular, we examine how a broad range of empirical phenomena in cognitive science (including intuitive physics, concept learning, causal reasoning, social cognition, and language understanding) can be modeled using a functional probabilistic programming language called Church.},
	Annote = {(<code>//probmods.org</code>)},
	Author = {Goodman, N. D. and Tenenbaum, J. B.},
	Date-Added = {2014-03-26 15:51:38 +0000},
	Date-Modified = {2014-03-26 15:53:06 +0000},
	Title = {Probabilistic Models of Cognition},
	Website = {https://probmods.org},
	Year = {2014}}

@unpublished{GoodmanGrant2013,
	Abstract = {Words are potentially one of the clearest windows on human knowledge and conceptual structure. But what do words mean? In this project we aim to construct and explore a formal model of lexical semantics grounded, via pragmatic inference, in core conceptual structures . Flexible human cognition is derived in large part from our ability to imagine possible worlds. A rich set of concepts, intuitive theories, and other mental representations support imagining and reasoning about possible worlds -- together we call these core cognition. Here we posit that the collection of core concepts also forms the set of primitive elements available for lexical semantics: word meanings are built from pieces of core cognition. We propose to study lexical semantics in the setting of an architecture for language understanding that integrates literal meaning with pragmatic inference. This architecture supports underspecified and uncertain lexical meaning, leading to subtle interactions between meaning, conceptual structure, and context. We will explore several cases of lexical semantics where these interactions are particularly important: indexicals, scalar adjectives, generics, and modals. We formalize both core cognition and the natural language architecture using the Church probabilistic programming language. In this project we aim to contribute to our understanding of the connection between words and mental representations; from this we expect to gain critical insights into many aspects of psychology, to construct vastly more useful thinking machines, and to interface natural and artificial intelligences more efficiently.},
	Annote = {(Unpublished manuscript.)},
	Author = {Goodman, N. D.},
	Title = {Grounding Lexical Meaning in Core Cognition},
	Website = {//www.stanford.edu/~ngoodman/papers/LexSemSquibb.pdf},
	Year = {2013}}

@article{FrankGoodmanICOM,
	Abstract = {Language comprehension is more than a process of decoding the literal meaning of a speaker's utterance. Instead, by making the assumption that speakers choose their words to be informative in context, listeners routinely make pragmatic inferences that go beyond the linguistic data. If language learners make these same assumptions, they should be able to infer word meanings in otherwise ambiguous situations. We use probabilistic tools to formalize these kinds of informativeness inferences -- extending a model of pragmatic language comprehension to the acquisition setting -- and present four experiments whose data suggest that preschool children can use informativeness to infer word meanings and that adult judgments track quantitatively with informativeness.},
	Author = {M. C. Frank and N. D. Goodman},
	Date-Added = {2012-01-29 18:09:51 -0800},
	Date-Modified = {2014-07-02 14:41:49 +0000},
	Journal = {Cognitive Psychology},
	Title = {Inferring word meanings by assuming that speakers are informative},
	Website = {//langcog.stanford.edu/papers/FG-cogpsych2014.pdf},
	Year = {2014}}

@article{GriffithsLiederGoodman,
	Annote = {(to appear)},
	Author = {T. L. Griffiths and F. Lieder and N. D. Goodman},
	Date-Added = {2013-08-17 19:50:56 +0000},
	Date-Modified = {2014-08-26 20:28:48 +0000},
	Journal = {Topics in Cognitive Science},
	Title = {Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic},
	Year = {To appear}}

@inbook{GoodmanLassiter,
	Author = {Noah D. Goodman and Daniel Lassiter},
	Booktitle = {The Handbook of Contemporary Semantic Theory, 2nd Edition},
	Date-Added = {2013-07-22 14:52:15 +0000},
	Date-Modified = {2013-08-17 20:28:19 +0000},
	Editor = {Shalom Lappin and Chris Fox},
	Publisher = {Wiley-Blackwell},
	Title = {Probabilistic Semantics and Pragmatics: Uncertainty in Language and Thought},
	Website = {//www.stanford.edu/~ngoodman/papers/Goodman-HCS-final.pdf},
	Year = {2015}}

@inbook{goodman2014concepts,
	Author = {Goodman, Noah D and Tenenbaum, Joshua B and Gerstenberg, T},
	Booktitle = {The Conceptual Mind: New Directions in the Study of Concepts},
	Date-Modified = {2014-07-01 15:37:34 +0000},
	Editor = {Morgolis and Lawrence},
	Publisher = {MIT Press},
	Title = {Concepts in a probabilistic language of thought},
	Website = {//www.stanford.edu/~ngoodman/papers/ConceptsChapter-final.pdf},
	Year = {2015}}

@article{Vul2014,
	Abstract = {In many learning or inference tasks human behavior approximates that of a Bayesian ideal observer, suggesting that, at some level, cognition can be described as Bayesian inference. However, a number of findings have highlighted an intriguing mismatch between human behavior and standard assumptions about optimality: People often appear to make decisions based on just one or a few samples from the appropriate posterior probability distribution, rather than using the full distribution. Although sampling-based approximations are a common way to implement Bayesian inference, the very limited numbers of samples often used by humans seem insufficient to approximate the required probability distributions very accurately. Here, we consider this discrepancy in the broader framework of statistical decision theory, and ask: If people are making decisions based on samples --  but as samples are costly -- how many samples should people use to optimize their total expected or worst-case reward over a large number of decisions? We find that under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods. These results help to reconcile a large body of work showing sampling-based or probability matching behavior with the hypothesis that human cognition can be understood in Bayesian terms, and they suggest promising future directions for studies of resource-constrained cognition.},
	Author = {E. Vul and N. D. Goodman and T. L. Griffiths and J. B. Tenenbaum},
	Date-Added = {2013-08-17 19:53:32 +0000},
	Date-Modified = {2014-03-14 16:48:57 +0000},
	Journal = {Cognitive Science},
	Title = {One and Done? Optimal Decisions From Very Few Samples},
	Website = {//www.stanford.edu/~ngoodman/papers/VulGoodmanGriffithsTenenbaum-COGS-2014.pdf},
	Year = {2014}}

@inproceedings{stuhlmuller2013learning,
	Abstract = {We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model's joint distribution: a factorization that turns observations into root nodes. Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly find a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples. To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler. We explore the efficiency of this sampler for a variety of parameter regimes and Bayes nets.},
	Author = {Stuhlm{\"u}ller, Andreas and Taylor, Jacob and Goodman, Noah},
	Booktitle = {Advances in Neural Information Processing Systems},
	Title = {Learning Stochastic Inverses},
	Website = {//www.stanford.edu/~ngoodman/papers/inverses-nips-2013.pdf},
	Year = {2013}}

@inproceedings{smith2013learning,
	Abstract = {Language users are remarkably good at making inferences about speakers' intentions in context, and children learning their native language also display substantial skill in acquiring the meanings of unknown words. These two cases are deeply related: Language users invent new terms in conversation, and language learners learn the literal meanings of words based on their pragmatic inferences about how those words are used. While pragmatic inference and word learning have both been independently characterized in probabilistic terms, no current work unifies these two. We describe a model in which language learners assume that they jointly approximate a shared, external lexicon and reason recursively about the goals of others in using this lexicon. This model captures phenomena in word learning and pragmatic inference; it additionally leads to insights about the emergence of communicative systems in conversation and the mechanisms by which pragmatic inferences become incorporated into word meanings.},
	Author = {Smith, Nathaniel J and Goodman, Noah and Frank, Michael},
	Booktitle = {Advances in Neural Information Processing Systems 26},
	Editor = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
	Pages = {3039--3047},
	Publisher = {Curran Associates, Inc.},
	Title = {Learning and using language via recursive pragmatic reasoning about other agents},
	Website = {//papers.nips.cc/paper/4929-learning-and-using-language-via-recursive-pragmatic-reasoning-about-other-agents.pdf},
	Year = {2013}}

@inproceedings{kaofunny,
	Abstract = {What makes something funny? Humor theorists posit that incongruity -- perceiving a situation from different viewpoints and finding the resulting interpretations to be incompatible -- contributes to sensations of mirth. In this paper, we use a computational model of sentence comprehension to formalize incongruity and test its relationship to humor in puns. By combining a noisy channel model of language comprehension and standard information theoretic measures, we derive two dimensions of incongruity -- ambiguity of meaning and distinctiveness of viewpoints -- and use them to predict humans' judgments of funniness. Results showed that both ambiguity and distinctiveness are significant predictors of humor. Additionally, our model automatically identifies specific features of a pun that make it amusing. We thus show how a probabilistic model of sentence comprehension can help explain essential features of the complex phenomenon of linguistic humor.},
	Author = {Kao, Justine T and Levy, Roger and Goodman, Noah D},
	Booktitle = {Proceedings of the Thirty-Fifth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {The Funny Thing About Incongruity: A Computational Model of Humor in Puns},
	Website = {//www.stanford.edu/~ngoodman/papers/KaoLevyGoodman.pdf},
	Year = {2013}}

@inproceedings{lieder2013learned,
	Abstract = {In learned helplessness experiments, subjects first experience a lack of control in one situation, and then show learning deficits when performing or learning another task in another situation. Generalization, thus, is at the core of the learned helplessness phenomenon. Substantial experimental and theoretical effort has been invested into establishing that a state- and task-independent belief about controllability is necessary. However, to what extent generalization is also sufficient to explain the transfer has not been examined. Here, we show qualitatively and quantitatively that Bayesian learning of action-outcome contingencies at three levels of abstraction is sufficient to account for the key features of learned helplessness, including escape deficits and impairment of appetitive learning after inescapable shocks.},
	Author = {Lieder, Falk and Goodman, Noah D and Huys, Quentin JM},
	Booktitle = {Proceedings of the Thirty-Fifth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Learned helplessness and generalization},
	Website = {//www.stanford.edu/~ngoodman/papers/LiederGoodmanHuys2013.pdf},
	Year = {2013}}

@misc{goodman2013principles,
	Annote = {(Extended abstract of keynote talk.)},
	Author = {Goodman, Noah D},
	Booktitle = {POPL 2013},
	Title = {The principles and practice of probabilistic programming},
	Website = {//www.stanford.edu/~ngoodman/papers/POPL2013-abstract.pdf},
	Year = {2013}}

@article{Goodman2013,
	Abstract = {Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to "invert" this model of the speaker. We apply this framework to model scalar implicature ("some" implies "not all," and "N" implies "not more than N"). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.},
	Author = {Goodman, N.D. and Stuhlm{\"u}ller, A.},
	Journal = {Topics in Cognitive Science},
	Page = {173--184},
	Title = {Knowledge and implicature: Modeling language understanding as social cognition},
	Volume = {5},
	Website = {//www.stanford.edu/~ngoodman/papers/GS-TopiCS-2013.pdf},
	Year = {2013}}

@inproceedings{Lassiter2013,
	Abstract = {Relative adjectives in the positive form exhibit vagueness and context sensitivity. We suggest that these phenomena can be explained by the interaction of a free threshold variable in the meaning of the positive form with a probabilistic model of pragmatic inference. We describe a formal model of utterance interpretation as coordination, which jointly infers the value of the threshold variable and the intended meaning of the sentence. We report simulations exploring the effect of background statistical knowledge on adjective interpretation in this model. Motivated by these simulation results, we suggest that this approach can account for the correlation between scale structure and the relative/absolute distinction while also allowing for exceptions noted in previous work. Finally, we argue for a probabilistic explanation of why the sorites paradox is compelling with relative adjectives even though the second premise is false on a universal interpretation, and show that this account predicts Kennedy's (2007) observation that the sorites paradox is more compelling with relative than with absolute adjectives.},
	Author = {D. Lassiter and N. D. Goodman},
	Booktitle = {Semantics and {L}inguistic {T}heory {(SALT)} 23},
	Date-Added = {2013-08-17 20:32:09 +0000},
	Date-Modified = {2013-08-17 20:33:01 +0000},
	Title = {Context, scale structure, and statistics in the interpretation of positive-form adjectives},
	Website = {//elanguage.net/journals/salt/article/view/23.587/3659},
	Year = {2013}}

@article{Stuhlmueller2013,
	Abstract = {A wide range of human reasoning patterns can be explained as conditioning in probabilistic models; however, conditioning has traditionally been viewed as an operation applied to such models, not represented in such models. We describe how probabilistic programs can explicitly represent conditioning as part of a model. This enables us to describe reasoning about others' reasoning using nested conditioning. Much of human reasoning is about the beliefs, desires, and intentions of other people; we use probabilistic programs to formalize these inferences in a way that captures the flexibility and inherent uncertainty of reasoning about other agents. We express examples from game theory, artificial intelligence, and linguistics as recursive probabilistic programs and illustrate how this representation language makes it easy to explore new directions in each of these fields. We discuss the algorithmic challenges posed by these kinds of models and describe how Dynamic Programming techniques can help address these challenges.},
	Author = {A. Stuhlm{\"u}ller and N. D. Goodman},
	Date-Added = {2013-08-17 17:09:29 +0000},
	Date-Modified = {2013-08-17 17:13:18 +0000},
	Journal = {J. Cognitive Systems Research},
	Title = {Reasoning about Reasoning by Nested Conditioning: Modeling Theory of Mind with Probabilistic Programs},
	Website = {//www.stanford.edu/~ngoodman/papers/StuhlmuellerGoodman-CogSys-2013.pdf},
	Year = {2013}}

@article{Hamlin2013,
	Abstract = {Evaluating individuals based on their proand anti-social behaviors is fundamental to successful human interaction. Recent research suggests that even preverbal infants engage in social evaluation; however, it remains an open question whether infants' judgments are driven uniquely by an analysis of the mental states that motivate others' helpful and unhelpful actions, or whether non-mentalistic inferences are at play. Here we present evidence from 10-month-olds, motivated and supported by a Bayesian computational model, for mentalistic social evaluation in the first year of life. A video abstract of this article can be viewed at <a href="//youtu.be/rD_Ry5oqCYE">//youtu.be/rD_Ry5oqCYE</a>},
	Author = {Hamlin, Kiley J and Ullman, Tomer and Tenenbaum, Josh B. and Goodman, Noah D. and Baker, Chris},
	Date-Added = {2013-08-17 20:15:14 +0000},
	Date-Modified = {2013-08-17 20:15:48 +0000},
	Journal = {{D}evelopmental {S}cience},
	Number = {2},
	Pages = {209--226},
	Publisher = {Wiley Online Library},
	Title = {The mentalistic basis of core social cognition: experiments in preverbal infants and a computational model},
	Volume = {16},
	Website = {//www.stanford.edu/~ngoodman/papers/HamlinUllmanetalDevSci2013.pdf},
	Year = {2013}}

@article{seiver2013did,
	Abstract = {Children rely on both evidence and prior knowledge to make physical causal inferences; this study explores whether they make attributions about others' behavior in the same manner. A total of one hundred and fifty-nine 4- and 6-year-olds saw 2 dolls interacting with 2 activities, and explained the dolls' actions. In the person condition, each doll acted consistently across activities, but differently from each other. In the situation condition, the two dolls acted differently for each activity, but both performed the same actions. Both age groups provided more "person" explanations (citing features of the doll) in the person condition than in the situation condition. In addition, 6-year-olds showed an overall bias toward "person" explanations. As in physical causal inference, social causal inference combines covariational evidence and prior knowledge.},
	Author = {Seiver, Elizabeth and Gopnik, Alison and Goodman, Noah D},
	Journal = {Child {D}evelopment},
	Number = {2},
	Pages = {443--454},
	Title = {Did she jump because she was the big sister or because the trampoline was safe? Causal inference and the development of social attribution},
	Volume = {84},
	Website = {//onlinelibrary.wiley.com/doi/10.1111/j.1467-8624.2012.01865.x/full},
	Year = {2013}}

@inproceedings{lieder2012burn,
	Abstract = {Bayesian inference provides a unifying framework for learning, reasoning, and decision making. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate timeaccuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We characterize the optimal time-accuracy tradeoff mathematically in terms of the number of iterations and the resulting bias as functions of time cost, error cost, and the difficulty of the inference problem. We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as "burn-in". Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions match published data on anchoring in numerical estimation tasks. In conclusion, resource-rationality -- the optimal use of finite computational resources -- naturally leads to a biased mind.},
	Author = {Lieder, Falk and Griffiths, Thomas L and Goodman, Noah D},
	Booktitle = {NIPS 2012},
	Pages = {2699--2707},
	Title = {Burn-in, bias, and the rationality of anchoring},
	Website = {//www.stanford.edu/~ngoodman/papers/LiederGriffithsGoodman2013NIPS.pdf},
	Year = {2012}}

@article{Ullman2012,
	Author = {T. Ullman and N. D. Goodman and J. B. Tenenbaum},
	Journal = {Cognitive {D}evelopment},
	Title = {Theory learning as stochastic search in the language of thought},
	Website = {//www.mit.edu/~tomeru/papers/tlss-final.pdf},
	Year = {2012}}

@inproceedings{stuhlmuller2012dynamic,
	Author = {Stuhlm{\"u}ller, Andreas and Goodman, Noah D},
	Journal = {Second Statistical Relational AI workshop at UAI 2012 (StaRAI-12)},
	Title = {A dynamic programming algorithm for inference in recursive probabilistic programs},
	Website = {//arxiv.org/abs/1206.3555},
	Year = {2012}}

@inproceedings{talton2012learning,
	Annote = {<b>[Nominated for Best Paper Award.]</b>},
	Author = {Talton, Jerry and Yang, Lingfeng and Kumar, Ranjitha and Lim, Maxine and Goodman, Noah D and Mech, R},
	Booktitle = {Proceedings of the 25th annual ACM symposium on User interface software and technology},
	Pages = {63--74},
	Title = {Learning design patterns with bayesian grammar induction},
	Website = {//www.stanford.edu/~ngoodman/papers/Talton12LDPBG.pdf},
	Year = {2012}}

@article{frank2012predicting,
	Author = {Frank, Michael C and Goodman, Noah D},
	Journal = {Science},
	Number = {6084},
	Pages = {998--998},
	Publisher = {American Association for the Advancement of Science},
	Title = {Predicting pragmatic reasoning in language games},
	Volume = {336},
	Website = {//www.stanford.edu/~ngoodman/papers/FrankGoodman-Science2012.pdf},
	Year = {2012}}

@book{tenenbaumreverseengineering,
	Annote = {(in prep)},
	Author = {J. B. Tenenbaum and T. L. Griffiths and N. Chater and C. Kemp and N. D. Goodman and A. Yuille},
	Title = {Reverse engineering the mind: the Bayesian approach},
	Year = {in prep}}

@inproceedings{yeh2012synthesizing,
	Author = {Yeh, Yi-Ting and Yang, Lingfeng and Watson, Matthew and Goodman, Noah D and Hanrahan, Pat},
	Journal = {SIGGRAPH 2012},
	Number = {4},
	Pages = {56},
	Title = {Synthesizing open worlds with constraints using locally annealed reversible jump MCMC},
	Volume = {31},
	Website = {//www.stanford.edu/~ngoodman/papers/owl.pdf},
	Year = {2012}}

@article{shafto2012learning,
	Author = {Shafto, Patrick and Goodman, Noah D and Frank, Michael C},
	Journal = {Perspectives on Psychological Science},
	Number = {4},
	Pages = {341--351},
	Publisher = {Sage Publications},
	Title = {Learning from others: The consequences of psychological reasoning for human learning},
	Volume = {7},
	Website = {//www.stanford.edu/~ngoodman/papers/shaftoperspectives.pdf},
	Year = {2012}}

@inproceedings{Goodman2012knowledge,
	Abstract = {Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to "invert" this model of the speaker. We apply this framework to model scalar implicature ("some" implies "not all," and "N" implies "not more than N"). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.},
	Annote = {<b>[Winner of the 2012 Cognitive Science Society computational modeling prize for Language.]</b>},
	Author = {Goodman, N.D. and Stuhlm{\"u}ller, A.},
	Date-Added = {2013-07-22 14:48:45 +0000},
	Date-Modified = {2013-07-22 14:48:56 +0000},
	Journal = {Proceedings of the Thirty-Fourth Annual Conference of the {C}ognitive {S}cience {S}ociety.},
	Title = {Knowledge and implicature: Modeling language understanding as social cognition},
	Website = {//www.stanford.edu/~ngoodman/papers/KnowledgeImplicature-v2.pdf},
	Year = {2012}}

@inproceedings{bergen2012s,
	Author = {Bergen, Leon and Goodman, Noah D and Levy, Roger},
	Journal = {Proceedings of the thirty-fourth annual conference of the {C}ognitive {S}cience {S}ociety},
	Title = {That's what she (could have) said: How alternative utterances affect language use},
	Website = {//www.stanford.edu/~ngoodman/papers/BergenEtAl2012.pdf},
	Year = {2012}}

@inproceedings{gerstenberg2012ping,
	Author = {Gerstenberg, Tobias and Goodman, Noah D},
	Journal = {Proceedings of the 34th annual conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Ping pong in Church: Productive use of concepts in human probabilistic inference},
	Website = {//www.stanford.edu/~ngoodman/papers/GerstenbergGoodman2012.pdf},
	Year = {2012}}

@inproceedings{gerstenberg2012noisy,
	Author = {Gerstenberg, Tobias and Goodman, Noah and Lagnado, David A and Tenenbaum, Joshua B},
	Booktitle = {Proceedings of the Thirty-Fourth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Noisy Newtons: Unifying process and dependency accounts of causal attribution},
	Website = {//www.stanford.edu/~ngoodman/papers/physics_cogsci2nd.pdf},
	Year = {2012}}

@inproceedings{lassiter2012many,
	Author = {Lassiter, Daniel and Goodman, Noah D},
	Journal = {34th Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {How many kinds of reasoning? Inference, probability, and natural language semantics},
	Website = {//www.stanford.edu/~ngoodman/papers/LassiterGoodman12.pdf},
	Year = {2012}}

@article{piantadosi2012bootstrapping,
	Author = {Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Cognition},
	Number = {2},
	Pages = {199--217},
	Title = {Bootstrapping in a language of thought: A formal model of numerical concept learning},
	Volume = {123},
	Website = {//www.stanford.edu/~ngoodman/papers/COGNIT2374.pdf},
	Year = {2012}}

@article{scontras2012comparing,
	Author = {Scontras, Gregory and Graff, Peter and Goodman, Noah D},
	Journal = {Cognition},
	Number = {1},
	Pages = {190--197},
	Title = {Comparing pluralities},
	Volume = {123},
	Website = {//www.stanford.edu/~ngoodman/papers/ScontrasGraffGoodman-ComparingPluralities.pdf},
	Year = {2012}}

@techreport{hwang2011inducing,
	Author = {Hwang, Irvin and Stuhlm{\"u}ller, Andreas and Goodman, Noah D},
	Journal = {Technical report: arXiv:1110.5667},
	Title = {Inducing probabilistic programs by Bayesian program merging},
	Website = {//arxiv.org/abs/1110.5667},
	Year = {2011}}

@article{cook2011science,
	Author = {Cook, Claire and Goodman, Noah D and Schulz, Laura E},
	Journal = {Cognition},
	Number = {3},
	Pages = {341--349},
	Title = {Where science starts: Spontaneous experiments in preschoolers' exploratory play},
	Volume = {120},
	Website = {//www.stanford.edu/~ngoodman/papers/CookGoodmanSchulz2011.pdf},
	Year = {2011}}

@article{bonawitz2011double,
	Author = {Bonawitz, Elizabeth and Shafto, Patrick and Gweon, Hyowon and Goodman, Noah D and Spelke, Elizabeth and Schulz, Laura},
	Journal = {Cognition},
	Number = {3},
	Pages = {322--330},
	Title = {The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery},
	Volume = {120},
	Website = {//cocolab.stanford.edu/papers/bonawitz-2011.pdf},
	Year = {2011}}

@article{chater2011imaginary,
	Annote = {(Commentary on Jones and Love.)},
	Author = {Chater, Nick and Goodman, Noah and Griffiths, Thomas L and Kemp, Charles and Oaksford, Mike and Tenenbaum, Joshua B},
	Journal = {Behavioral and Brain Sciences},
	Number = {04},
	Pages = {194--196},
	Publisher = {Cambridge University Press},
	Title = {The imaginary fundamentalists: The unshocking truth about Bayesian cognitive science},
	Volume = {34},
	Website = {//journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=8359950},
	Year = {2011}}

@inproceedings{wingate2011bayesian,
	Annote = {<b>[Winner of the Best Poster prize]</b>},
	Author = {Wingate, David and Goodman, Noah D and Roy, Daniel M and Kaelbling, Leslie P and Tenenbaum, Joshua B},
	Booktitle = {Proceedings of the Twenty-Second international joint conference on Artificial Intelligence (IJCAI 11)},
	Title = {Bayesian policy search with policy priors},
	Website = {//www.stanford.edu/~ngoodman/papers/WingateEtAl-PolicyPrios.pdf},
	Year = {2011}}

@article{tenenbaum2011grow,
	Author = {Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L and Goodman, Noah D},
	Journal = {Science},
	Number = {6022},
	Pages = {1279--1285},
	Title = {How to grow a mind: Statistics, structure, and abstraction},
	Volume = {331},
	Website = {//www.stanford.edu/~ngoodman/papers/tkgg-science11-reprint.pdf},
	Year = {2011}}

@inproceedings{wingate2011nonstandard,
	Author = {Wingate, David and Goodman, Noah D and Stuhlmueller, Andreas and Siskind, Jeffrey Mark},
	Booktitle = {Advances in Neural Information Processing Systems 23},
	Pages = {1152--1160},
	Title = {Nonstandard Interpretations of Probabilistic Programs for Efficient Inference},
	Website = {//www.stanford.edu/~ngoodman/papers/WGSS-NIPS11.pdf},
	Year = {2011}}

@inproceedings{stiller2011ad,
	Author = {Stiller, Alex and Goodman, Noah D and Frank, Michael C},
	Journal = {Proceedings of the 33rd Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Ad-hoc scalar implicature in adults and children},
	Website = {//www.stanford.edu/~ngoodman/papers/SGF-cogsci2011.pdf},
	Year = {2011}}

@inproceedings{o2011productivity,
	Annote = {<b>[Winner of the 2011 Cognitive Science Society computational modeling prize for Language.]</b>},
	Author = {O'donnell, Timothy J and Snedeker, Jesse and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Proceedings of the Thirty-Third Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Productivity and reuse in language},
	Website = {//www.stanford.edu/~ngoodman/papers/odonnell-cogsci11.pdf},
	Year = {2011}}

@inproceedings{Wingate2011a,
	Abstract = {We describe a general method of transforming arbitrary programming languages into probabilistic programming languages with straightforward MCMC inference engines. Random choices in the program are "named" with information about their position in an execution trace; these names are used in conjunction with a database holding values of random variables to implement MCMC inference in the space of execution traces. We encode naming information using lightweight source-to-source compilers. Our method enables us to reuse existing infrastructure (compilers, profilers, etc.) with minimal additional code, implying fast models with low development overhead. We illustrate the technique on two languages, one functional and one imperative: Bher, a compiled version of the Church language which eliminates interpretive overhead of the original MIT-Church implementation, and Stochastic Matlab, a new open-source language.},
	Author = {Wingate, D. and Stuhlm{\"u}ller, A. and Goodman, N.D.},
	Booktitle = {Proceedings of the 14th international conference on Artificial Intelligence and Statistics},
	Pages = {770--778},
	Title = {Lightweight implementations of probabilistic programming languages via transformational compilation},
	Website = {//jmlr.csail.mit.edu/proceedings/papers/v15/wingate11a/wingate11a.pdf},
	Year = {2011}}

@article{goodman2011learning,
	Abstract = {The very early appearance of abstract knowledge is often taken as evidence for innateness. We explore the relative learning speeds of abstract and specific knowledge within a Bayesian framework, and the role for innate structure. We focus on knowledge about causality, seen as a domain-general intuitive theory, and ask whether this knowledge can be learned from cooccurrence of events. We begin by phrasing the causal Bayes nets theory of causality, and a range of alternatives, in a logical language for relational theories. This allows us to explore simultaneous inductive learning of an abstract theory of causality and a causal model for each of several causal systems. We find that the correct theory of causality can be learned relatively quickly, often becoming available before specific causal theories have been learned -- an effect we term the blessing of abstraction. We then explore the effect of providing a variety of auxiliary evidence, and find that a collection of simple "perceptual input analyzers" can help to bootstrap abstract knowledge. Together these results suggest that the most efficient route to causal knowledge may be to build in not an abstract notion of causality, but a powerful inductive learning mechanism and a variety of perceptual supports. While these results are purely computational, they have implications for cognitive development, which we explore in the conclusion.},
	Author = {Goodman, Noah D and Ullman, Tomer D and Tenenbaum, Joshua B},
	Journal = {Psychological {R}eview},
	Number = {1},
	Pages = {110},
	Publisher = {American Psychological Association},
	Title = {Learning a theory of causality.},
	Volume = {118},
	Website = {//www.stanford.edu/~ngoodman/papers/LTBC_psychreview_final.pdf},
	Year = {2011}}

@article{desrochers2010optimal,
	Author = {Desrochers, Theresa M and Jin, Dezhe Z and Goodman, Noah D and Graybiel, Ann M},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {47},
	Pages = {20512--20517},
	Publisher = {National Acad Sciences},
	Title = {Optimal habits can develop spontaneously through sensitivity to local cost},
	Volume = {107},
	Website = {//www.pnas.org/content/107/47/20512},
	Year = {2010}}

@article{kemp2010learning,
	Author = {Kemp, Charles and Goodman, Noah D and Tenenbaum, Joshua B},
	Journal = {Cognitive Science},
	Number = {7},
	Pages = {1185--1243},
	Publisher = {Blackwell Publishing Ltd},
	Title = {Learning to learn causal models},
	Volume = {34},
	Website = {//www.psy.cmu.edu/~ckemp/papers/kempgt10_learningtolearncausalmodels.pdf},
	Year = {2010}}

@article{frank2010predicting,
	Author = {Frank, Michael and Kenney, Avril and Goodman, Noah and Tenenbaum, Joshua and Torralba, Antonio and Oliva, Aude},
	Journal = {Journal of Vision},
	Number = {7},
	Pages = {1241--1241},
	Publisher = {Association for Research in Vision and Ophthalmology},
	Title = {Predicting object and scene descriptions with an information-theoretic model of pragmatics},
	Volume = {10},
	Website = {//171.67.113.220/content/10/7/1241.short},
	Year = {2010}}

@article{henderson2010structure,
	Author = {Henderson, Leah and Goodman, Noah D and Tenenbaum, Joshua B and Woodward, James F},
	Date-Modified = {2015-04-02 13:10:56 +0000},
	Journal = {Philosophy of Science},
	Number = {2},
	Pages = {172--200},
	Title = {The Structure and Dynamics of Scientific Theories: A Hierarchical Bayesian Perspective},
	Volume = {77},
	Website = {//cocolab.stanford.edu/papers/Henderson2010.pdf},
	Year = {2010}}

@inproceedings{shafto2010prior,
	Author = {Shafto, Patrick and Goodman, Noah D and Gerstle, Ben and Ladusaw, Francy},
	Journal = {32nd annual conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Prior expectations in pedagogical situations},
	Website = {//mindmodeling.org/cogsci2010/papers/0514/paper0514.pdf},
	Year = {2010}}

@inproceedings{stuhlmuller2010learning,
	Author = {Stuhlm{\"u}ller, Andreas and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Proceedings of the Thirty-Second Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Learning structured generative concepts},
	Website = {//www.mit.edu/~ast/papers/structured-generative-concepts-cogsci2010.pdf},
	Year = {2010}}

@inproceedings{piantadosi2010beyond,
	Author = {Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Proceedings of the 32nd Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Pages = {859--864},
	Title = {Beyond Boolean logic: exploring representation languages for learning complex concepts},
	Website = {//colala.bcs.rochester.edu/papers/PiantadosiTenenbaumGoodman2010-CogsciFINAL.pdf},
	Year = {2010}}

@inproceedings{Ullman2010,
	Author = {Ullman, T.D. and Goodman, N.D. and Tenenbaum, J.B.},
	Booktitle = {Proceedings of Thirty Second Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Theory Acquisition as Stochastic Search},
	Website = {//www.stanford.edu/~ngoodman/papers/tlss_2010_final.pdf},
	Year = {2010}}

@inproceedings{wingate2009infinite,
	Author = {Wingate, David and Goodman, Noah D and Roy, Daniel M and Tenenbaum, Joshua B},
	Booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
	Organization = {AUAI Press},
	Pages = {607--614},
	Title = {The infinite latent events model},
	Website = {//web.mit.edu/~wingated/www/papers/ilem.pdf},
	Year = {2009}}

@inproceedings{Goodman2009,
	Author = {Goodman, N. D. and Ullman, T. D. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of the 31st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2009-11-01 16:09:15 -0500},
	Owner = {noah},
	Timestamp = {2009.03.08},
	Title = {Learning a theory of causality},
	Website = {//stanford.edu/~ngoodman/papers/LTBC_v2.pdf},
	Year = {2009}}

@inproceedings{Vul2009,
	Abstract = {In many learning or inference tasks human behavior approximates that of a Bayesian ideal observer, suggesting that, at some level, cognition can be described as Bayesian inference. However, a number of findings have highlighted an intriguing mismatch between human behavior and standard assumptions about optimality: People often appear to make decisions based on just one or a few samples from the appropriate posterior probability distribution, rather than using the full distribution. Although sampling-based approximations are a common way to implement Bayesian inference, the very limited numbers of samples often used by humans seem insufficient to approximate the required probability distributions very accurately. Here, we consider this discrepancy in the broader framework of statistical decision theory, and ask: If people are making decisions based on samples -- but as samples are costly -- how many samples should people use to optimize their total expected or worst-case reward over a large number of decisions? We find that under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods. These results help to reconcile a large body of work showing sampling-based or probability matching behavior with the hypothesis that human cognition can be understood in Bayesian terms, and they suggest promising future directions for studies of resource-constrained cognition.},
	Author = {E. Vul and N. D. Goodman and T. L. Griffiths and J. B. Tenenbaum},
	Booktitle = {Proceedings of the 31st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2009-11-02 16:37:59 -0500},
	Date-Modified = {2009-11-02 16:38:55 -0500},
	Title = {One and done: Globally optimal behavior from locally suboptimal decisions.},
	Website = {//stanford.edu/~ngoodman/papers/VulEtAl2009.pdf},
	Year = {2009}}

@article{Frank2009,
	Author = {Frank, M. C. and Goodman, N. D. and Tenenbaum, J. B.},
	Date-Modified = {2013-08-19 22:52:10 +0000},
	Journal = {Psychological Science},
	Owner = {noah},
	Timestamp = {2009.03.08},
	Title = {Using speakers' referential intentions to model early cross-situational word learning},
	Website = {//langcog.stanford.edu/papers/FGT-psychscience2009.pdf},
	Year = {2009}}

@techreport{o2009fragment,
	Author = {O'Donnell, Timothy J and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Technical Report MIT-CSAIL-TR-2009-013},
	Publisher = {Massachusetts Institute of Technology},
	Title = {Fragment grammars: Exploring computation and reuse in language},
	Website = {//dspace.mit.edu/handle/1721.1/44963},
	Year = {2009}}

@inproceedings{schmidt2009tall,
	Author = {Schmidt, Lauren A and Goodman, Noah D and Barner, David and Tenenbaum, Joshua B},
	Journal = {Proceedings of the 31st annual conference of the {C}ognitive {S}cience {S}ociety},
	Pages = {2759--2764},
	Title = {How tall is Tall? compositionality, statistics, and gradable adjectives},
	Website = {//stanford.edu/~ngoodman/papers/SchmidtEtAl2009.pdf},
	Year = {2009}}

@inproceedings{frank2009continuity,
	Author = {Frank, Michael C and Goodman, Noah D and Tenenbaum, Joshua B and Fernald, Anne},
	Journal = {Proceedings of the 31st Annual {C}ognitive {S}cience {S}ociety},
	Title = {Continuity of discourse provides information for word learning},
	Website = {//stanford.edu/~ngoodman/papers/FrankGoodmanTenenbaumFernald2009.pdf},
	Year = {2009}}

@inproceedings{Goodman2009a,
	Author = {Goodman, N. D. and Baker, C. L. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of the 31st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2009-11-01 16:10:03 -0500},
	Owner = {noah},
	Timestamp = {2009.03.08},
	Title = {Cause and intent: Social reasoning in causal learning},
	Website = {//stanford.edu/~ngoodman/papers/SocCause_v1.pdf},
	Year = {2009}}

@inproceedings{Ullman2009,
	Author = {T. Ullman and C. L. Baker and O. Macindoe and O. Evans and N. D. Goodman and J. B. Tenenbaum},
	Booktitle = {Advances in Neural Information Processing Systems 22},
	Date-Added = {2009-11-01 16:10:11 -0500},
	Date-Modified = {2012-01-29 18:13:46 -0800},
	Title = {Help or hinder: Bayesian models of social goal inference.},
	Website = {//web.mit.edu/tomeru/www/papers/nips2010.pdf},
	Year = {2009}}

@inproceedings{Frank2009a,
	Author = {M. C. Frank and N. D. Goodman and P. Lai and J. B. Tenenbaum},
	Booktitle = {Proceedings of the 31st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2009-11-01 16:15:07 -0500},
	Date-Modified = {2009-11-01 16:16:02 -0500},
	Title = {Informative communication in word production and word learning},
	Website = {//stanford.edu/~ngoodman/papers/FrankGoodmanLaiTenenbaum2009.pdf},
	Year = {2009}}

@techreport{McAllester2008,
	Author = {D. McAllester and B. Milch and N. D. Goodman},
	Institution = {Massachusetts Institute of Technology},
	Number = {MIT-CSAIL-TR-2008-025},
	Owner = {noah},
	Timestamp = {2008.08.30},
	Title = {Random-World Semantics and Syntactic Independence for Expressive Languages},
	Website = {//stanford.edu/~ngoodman/papers/logical-dimred.pdf},
	Year = {2008}}

@article{Schulz2008,
	Abstract = {Given minimal evidence about novel objects, children might learn only
	relationships among the specific entities, or they might make a more
	abstract inference, positing classes of entities and the relations
	that hold among those classes. Here we show that preschoolers (mean:
	57 months) can use sparse data about perceptually unique objects
	to infer abstract physical causal laws. These newly inferred abstract
	laws were robust to potentially anomalous evidence; in the face of
	apparent counter-evidence, children (correctly) posited the existence
	of an unobserved object rather than revise the abstract laws. This
	suggests that children's ability to learn robust, abstract principles
	does not depend on extensive prior experience but can occur rapidly,
	on-line, and in tandem with inferences about specific relations.},
	Author = {Laura E. Schulz and Noah D. Goodman and Joshua B. Tenenbaum and Adrianna C. Jenkins},
	Doi = {n.2008.07.017},
	Journal = {Cognition},
	Month = {Nov},
	Number = {2},
	Owner = {noah},
	Pages = {211--223},
	Pii = {S0010-0277(08)00179-0},
	Pmid = {18930186},
	Timestamp = {2009.01.09},
	Title = {Going beyond the evidence: abstract laws and preschoolers' responses to anomalous data.},
	Volume = {109},
	Website = {//eccl.mit.edu/papers/SchulzGoodman_etal.pdf},
	Year = {2008},
	Bdsk-Url-1 = {//dx.doi.org/2008.07.017}}

@misc{roy2008stochastic,
	Author = {Roy, DM and Mansinghka, VK and Goodman, ND and Tenenbaum, JB},
	Journal = {Nonparametric Bayesian Workshop, Int. Conf. on Machine Learning},
	Pages = {26},
	Title = {A stochastic programming perspective on nonparametric Bayes},
	Volume = {22},
	Website = {//danroy.org/papers/RoyManGooTen-ICMLNPB-2008.pdf},
	Year = {2008}}

@inproceedings{Katz2008,
	Author = {Katz, Y. and Goodman, N. D. and Kersting, K. and Kemp, C. and Tenenbaum, J. B.},
	Journal = {Proceedings of the Thirtieth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Modeling Semantic Cognition as Logical Dimensionality Reduction},
	Website = {//stanford.edu/~ngoodman/papers/logical-dimred.pdf},
	Year = {2008}}

@inbook{Goodman2008compositionality,
	Author = {Noah D. Goodman and Joshua B. Tenenbaum and Thomas L. Griffiths and Jacob Feldman},
	Booktitle = {The probabilistic mind: Prospects for rational models of cognition},
	Editor = {Michael Oaksford and Nick Chater},
	Owner = {noah},
	Publisher = {Oxford University Press},
	Timestamp = {2007.03.21},
	Title = {Compositionality in rational analysis: Grammar-based induction for concept learning},
	Website = {//www.stanford.edu/~ngoodman/papers/OaksfordChaterChapter_final.pdf},
	Year = {2008}}

@inproceedings{Mayrhofer2008,
	Author = {Mayrhofer, R. and Goodman, N. D. and Waldmann, M. R. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of the Thirtieth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Structured Correlation from the Causal Background},
	Website = {//stanford.edu/~ngoodman/papers/fp906-mayrhofer.pdf},
	Year = {2008}}

@inproceedings{Baker2008,
	Author = {Baker, C. L. and Goodman, N. D. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of Thirtieth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Owner = {noah},
	Timestamp = {2009.03.08},
	Title = {Theory-based Social Goal Inference},
	Website = {//web.mit.edu/clbaker/www/papers/cogsci2008.pdf},
	Year = {2008}}

@inproceedings{Kemp2008,
	Author = {Kemp, C. and Goodman, N. D. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of Thirtieth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Theory acquisition and the language of thought},
	Website = {//stanford.edu/~ngoodman/papers/logiclaws.pdf},
	Year = {2008}}

@inproceedings{Piantadosi2008,
	Author = {Piantadosi, S. T. and Goodman, N. D. and Ellis, B. A. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of Thirtieth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {A Bayesian Model of the Acquisition of Compositional Semantics},
	Website = {//stanford.edu/~ngoodman/papers/pp826-piantadosi.pdf},
	Year = {2008}}

@inproceedings{Shafto2008,
	Author = {Patrick Shafto and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirtieth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Owner = {noah},
	Timestamp = {2007.04.15},
	Title = {Teaching Games: Statistical Sampling Assumptions for Learning in Pedagogical Situations},
	Website = {//stanford.edu/~ngoodman/papers/pedagogicalSampling.pdf},
	Year = {2008}}

@inproceedings{Goodman2008,
	Author = {N. D. Goodman and V. K. Mansinghka and D. M. Roy and K. Bonawitz and J. B. Tenenbaum},
	Journal = {Uncertainty in Artificial Intelligence},
	Title = {Church: a language for generative models},
	Website = {//stanford.edu/~ngoodman/papers/churchUAI08_rev2.pdf},
	Year = {2008}}

@article{Goodman2008a,
	Author = {Noah D. Goodman and Joshua B. Tenenbaum and Jacob Feldman and Thomas L. Griffiths},
	Journal = {Cognitive Science},
	Number = {1},
	Owner = {noah},
	Pages = {108---154},
	Timestamp = {2007.06.26},
	Title = {A Rational Analysis of Rule-based Concept Learning},
	Volume = {32},
	Website = {//stanford.edu/~ngoodman/papers/RRfinal3.pdf},
	Year = {2008}}

@inproceedings{Frank2007,
	Author = {Frank, M. C. and Goodman, N. D. and Tenenbaum, J. B.},
	Journal = {Advances in Neural Information Processing Systems},
	Publisher = {MIT Press},
	Title = {A bayesian framework for crosssituational word-learning},
	Volume = {20},
	Website = {//langcog.stanford.edu/papers/frank-nips-2007.pdf},
	Year = {2007}}

@inproceedings{kemp2007learning,
	Author = {Kemp, Charles and Goodman, Noah D and Tenenbaum, Joshua B},
	Booktitle = {NIPS},
	Title = {Learning and using relational theories.},
	Website = {https://papers.nips.cc/paper/3332-learning-and-using-relational-theories.pdf},
	Year = {2007}}

@inproceedings{Goodman2007c,
	Annote = {<b>[Winner of the 1007 Cognitive Science Society computational modeling prize for Perception and Action.]</b>},
	Author = {Noah D. Goodman and Vikash Mansinghka and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Twenty-Ninth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Owner = {noah},
	Timestamp = {2007.11.17},
	Title = {Learning grounded causal models},
	Website = {//stanford.edu/~ngoodman/papers/op510-goodman.pdf},
	Year = {2007}}

@inproceedings{Kemp2007,
	Annote = {<b>[Winner of the 2007 Cognitive Science Society computational modeling prize for Higher-level Cognition.]</b>},
	Author = {Charles Kemp and Noah D. Goodman and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Twenty-ninth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Owner = {noah},
	Timestamp = {2007.04.15},
	Title = {Learning causal schemata},
	Website = {//stanford.edu/~ngoodman/papers/KempGT07.pdf},
	Year = {2007}}

@inproceedings{Goodman2006,
	Author = {Noah D. Goodman and Chris L. Baker and Elizabeth Baraff-Bonawitz and Vikash K. Mansinghka and Alison Gopnik and Henry Wellman and Laura Schulz and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Twenty-Eight Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Owner = {noah},
	Timestamp = {2008.09.01},
	Title = {Intuitive theories of mind: a rational approach to false belief},
	Website = {//stanford.edu/~ngoodman/papers/pos785-goodman.pdf},
	Year = {2006}}
