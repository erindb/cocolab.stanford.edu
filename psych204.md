---
layout: default
---

#Computation and Cognition: the Probabilistic Approach (psych 204, spring 2015)

##Overview

This course will introduce the probabilistic approach to cognitive science, in which learning and reasoning are understood as inference in complex probabilistic models. Examples will be drawn from areas including concept learning, causal reasoning, social cognition, and language understanding. Formal modeling ideas and techniques will be discussed in concert with relevant empirical phenomena.

Instructor: Noah Goodman (ngoodman at stanford dot edu)  
TAs: Desmond Ong (dco at stanford dot edu) and Michael Tessler (mtessler at stanford dot edu)  
Meeting time: Tue, Thu 3:15 - 4:30  
Meeting place: Littlefield 103  
Office hours: 

* Desmond: Thursdays 11-12pm, Jordan Hall Room 330
* Michael: Mondays 3-4pm, Jordan Hall Room 316
* Instructor will meet with students after class or by appointment.

Link to Piazza signup: piazza.com/stanford/spring2015/psych204

We encourage students to use Piazza for questions that are readily articulable. Piazza is a forum for questions & answers, and answers may be posed by both students and TAs. We strongly encourage students to answer questions, and TAs will verify the solutions.

##Assignments and grading
Students (both registered and auditing) will be expected to do assigned readings before class.
Registered students will be graded based on:

* 20% Class participation and Piazza participation.
* 30% Homework.
* 50% Final project (including proposal, update, presentation, and paper).

Send all assignment submissions and correspondences to psych204-spr1415-staff at lists dot stanford dot edu

Assignments should be in .pdf form; fixed-width font appreciated. Assignments will be graded using the following scheme:

After the first attempt of the problem set:

+ Full credit: Assignment is complete and solutions are correct
+ Half credit: Assignment was attempted, with incorrect solutions
+ No credit: Assignment was not attempted

You will receive feedback on your work. If you receive *half credit*, you will have the opportunity for revision:

+ Full credit (revised attempt): Assignment is complete, solutions are correct, and explanations for why original solution was incorrect are provided

##Readings
Readings for each week will be linked from the calendar below. (In some cases these will require an SUNet ID to access. See the instructor in case of trouble.) Readings will be drawn from the webbook [Probabilistic Models of Cognition](http://probmods.org/) and selected research papers.

##Pre-requisites
There are no formal pre-requisites for this class. However, this is a graduate-level course, which will move relatively quickly and have technical content. Students should be already familiar with the basics of probability and programming (or be willing to learn this background on their own).




#Schedule

##Week of March 31
Introduction. Simulation, computation, and generative models. Probability and belief.

Homework: Excercises on Scheme Basics and Generative Models.

Readings:

 * [Scheme Basics](http://probmods.org/appendix-scheme.html)
 * [Generative Models](http://probmods.org/generative-models.html)
 * [Concepts in a probabilistic language of thought.](http://www.stanford.edu/~ngoodman/papers/ConceptsChapter-final.pdf) Goodman, Tenenbaum, Gerstenberg (2015).
 * Optional: [How to grow a mind: structure, statistics, and abstraction.](../papers/tkgg-science11-reprint.pdf), J. B. Tenenbaum, C. Kemp, T. L. Griffiths, and N. D. Goodman (2011).
 * Optional: [Ping Pong in Church: Productive use of concepts in human probabilistic inference.](http://stanford.edu/~ngoodman/papers/GerstenbergGoodman2012.pdf) Gerstenberg and Goodman (2012).
 * Optional: [Structure and Interpretation of Computer Programs.](http://mitpress.mit.edu/sicp/) (This is an amazing intro to computer science, through Scheme.)
 * Optional: [Some](http://www.shido.info/lisp/idx_scm_e.html) [Scheme](http://www.cs.hut.fi/Studies/T-93.210/schemetutorial/schemetutorial.html) [tutorials](http://www.ccs.neu.edu/home/dorai/t-y-scheme/t-y-scheme.html).
 * Optional: [Internal physics models guide probabilistic judgments about object dynamics.](http://web.mit.edu/~pbatt/www/publications/HamrBattTene11CogSci33.pdf) Hamrick, Battaglia, Tenenbaum (2011).
 * Optional: [Sources of uncertainty in intuitive physics.](http://www.edvul.com/pdf/SV-CogSci-2012.pdf) Smith and Vul (2012).


##Week of April 7
Conditioning and inference. Causal vs. statistical dependency. Patterns of inference. 

Homework: Excercises on Conditioning and Patterns of Inference.

Readings:

* [Conditioning](http://probmods.org/conditioning.html)
* [Patterns of Inference](http://probmods.org/patterns-of-inference.html)
* [Predicting the future.](http://cocosci.berkeley.edu/tom/papers/predictions.pdf) Griffiths and Tenenbaum (2006).
* Optional: [Causal Reasoning Through Intervention.](http://www.ucl.ac.uk/lagnado-lab/publications/lagnado/intervention%20hagmayer%20et%20al.pdf) Hagmayer, Sloman, Lagnado, and Waldmann (2006).
* Optional: [Children's causal inferences from indirect evidence: Backwards blocking and Bayesian reasoning in preschoolers.](restrictedpapers/Sobel2004.pdf) Sobel, Tenenbaum, Gopnik (2004).
* Optional: [Bayesian models of object perception.](http://vision.psych.umn.edu/users/kersten/kersten-lab/papers/KerstenYuilleApr2003.pdf) Kersten and Yuille (2003).



## Week of April 14
Sequences of observations. Bayesian data analysis. Discussion on levels of analysis.

Homework: Excercises on Bayesian data analysis.

<!--[Mini-project](https://probmods.org/mini-projects.html#explaining-away) -->

Readings:

* [Models for sequences of observations](http://probmods.org/observing-sequences.html)
* [Bayesian data analysis]()
* [Chapter 1 of "The adaptive character of thought."](restrictedpapers/Anderson90.pdf) Anderson (1990).
* Optional: [Chapter 1 of  "Vision."](restrictedpapers/MarrCh1.pdf) Marr (1982).
* Optional: [Ten Years of Rational Analysis.](restrictedpapers/chater99ten.pdf) Chater, Oaksford (1999).
* Optional: [The Knowledge Level.](restrictedpapers/Newell1982.pdf) Newell (1982).



## Week of April 21
Social cognition.

Homework: Excercises on Inference about Inference, also work on [project proposals]().

Readings:

* [Inference about Inference](http://probmods.org/inference-about-inference.html)
* Optional: [Goal Inference as Inverse Planning.](http://web.mit.edu/clbaker/www/papers/cogsci2007.pdf) Baker, Tenenbaum, Saxe (2007).
* Optional: [Cause and intent: Social reasoning in causal learning.](http://stanford.edu/~ngoodman/papers/SocCause_v1.pdf) Goodman, Baker, Tenenbaum (2009).
* Optional: [Reasoning about Reasoning by Nested Conditioning: Modeling Theory of Mind with Probabilistic Programs.](http://stanford.edu/~ngoodman/papers/StuhlmuellerGoodman-CogSys-2013.pdf) Stuhlmueller and Goodman (2013).
* Optional: [Young children use statistical sampling to infer the preferences of other people.](https://courses.cit.cornell.edu/tk397/ECCL/Publications_files/Psychological%20Science-2010-Kushnir-0956797610376652.pdf) Kushnir, Xu, and Wellman (2010).


## Week of April 28
Natural language pragmatics and semantics.
<p><a href="projects.html">Project proposals</a> due Friday!</p>

Readings:

* [Probabilistic Semantics and Pragmatics: Uncertainty in Language and Thought](http://www.stanford.edu/~ngoodman/papers/Goodman-HCS-final.pdf) Goodman and Lassiter (2015).
* [Quantifying pragmatic inference in language games.](http://stanford.edu/~ngoodman/papers/FrankGoodman-Science2012.pdf) Frank and Goodman (2012).
* Something on noisy chanels.
* Optional: [Teaching games: statistical sampling assumptions for learning in pedagogical situations.](http://stanford.edu/~ngoodman/papers/pedagogicalSampling.pdf) Shafto and Goodman (2008).
* Optional: <a href="http://stanford.edu/~ngoodman/papers/GS-TopiCS-2013.pdf">Knowledge and implicature: Modeling language understanding as social cognition.</a> Goodman and Stuhlmueller (2013).



## Week of May 5
Learning as inference.

Readings:

* [Learning as Conditional Inference](https://probmods.org/learning-as-conditional-inference.html)
* [A rational analysis of rule-based concept learning.](http://stanford.edu/~ngoodman/papers/RRfinal3.pdf) Goodman, Tenenbaum, Feldman, and Griffiths (2008).
* Optional: <a href="http://web.mit.edu/cocosci/Papers/nips99preprint.pdf">Rules and similarity in concept learning.</a> Tenenbaum (2000).
* Optional: <a href="http://www.mit.edu/~ast/papers/structured-generative-concepts-cogsci2010.pdf">Learning Structured Generative Concepts.</a> Stuhlmueller, Tenenbaum, and Goodman (2010).


## Week of May 12
Hierarchical models. Mixture models. Occam's razor.

Project update (preliminary paper) due on Friday at midnight!

Readings:

* <a href="https://probmods.org/hierarchical-models.html">Hierarchical Models</a>
* <a href="https://probmods.org/occam's-razor.html">Occam's Razor</a>
* <a href="http://web.mit.edu/cocosci/Papers/structure-strength-reprint.pdf">Structure and strength in causal induction.</a> Griffiths and Tenenbaum (2005).
* Optional: <a href="http://web.mit.edu/cocosci/Papers/bayes.pdf">Bayesian modeling of human concept learning.</a> Tenenbaum (1999).
* Optional: <a href="http://web.mit.edu/cocosci/Papers/cogsci00_FINAL.pdf">Word learning as Bayesian inference.</a> Tenenbaum and Xu (2000).
* Optional: <a href="http://web.mit.edu/cocosci/Papers/f881-XuTenenbaum.pdf"> Word learning as Bayesian inference: Evidence from preschoolers.</a> Xu and Tenenbaum (2005).
* Optional: <a href="http://www.psy.cmu.edu/~ckemp/papers/KempPT06.pdf">Learning overhypotheses.</a> Kemp, Perfors, and Tenenbaum (2006).
* Optional: <a>Object name learning provides on-the-job training for attention.</a> Smith, Jones, Landau, Gershko-Stowe, and Samuelson (2002).



## Week of May 26
Inference algorithms and resource-rational process models.

Readings:

* [Algorithms for Inference](http://probmods.org/inference-process.html)
* [One and done: Globally optimal behavior from locally suboptimal decisions.](http://stanford.edu/~ngoodman/papers/VulEtAl2009.pdf) Vul, Goodman, Griffiths, Tenenbaum (2009).
* <a href="http://www.stanford.edu/~ngoodman/papers/LiederGriffithsGoodman2013NIPS.pdf">Burn-in, bias, and the rationality of anchoring.</a> Lieder, Griffiths, and Goodman (2012).
* Optional: <a href="http://www.princeton.edu/~sjgershm/GershmanVulTenenbaum09.pdf">Perceptual multistability as Markov chain Monte Carlo inference.</a> Gershman, Vul, Tenenbaum (2009).
* Optional: <a href="http://cocosci.berkeley.edu/tom/papers/rational1.pdf">A more rational model of categorization.</a> Sanborn, Griffiths,  Navarro (2006).
* Optional: <a href="http://stanford.edu/~ngoodman/papers/tlss_2010_final.pdf">Theory acquisition as stochastic search.</a> Ullman, Goodman, and Tenenbaum (2010).
* Optional: <a href="http://cocosci.berkeley.edu/tom/papers/mechanism.pdf">Exemplar models as a mechanism for performing Bayesian inference.</a> Shi, Griffiths, Feldman, Sanborn (2010).


## Week of May 19
Catch up, non-parametric models, other topics.

<!--
<li> <a href="http://www.psy.cmu.edu/~ckemp/papers/KempTGYU06.pdf">Learning systems of concepts with an infinite relational model.</a> Kemp, C., Tenenbaum, J. B., Griffiths, T. L., Yamada, T., Ueda, N. (2006).
<li> Optional: <a href="http://www.psy.cmu.edu/~ckemp/papers/kempgt10_learningtolearncausalmodels.pdf">Learning to learn causal models.</a> Kemp, C., Goodman, N., Tenenbaum, J. (2010).
-->

## June 2
Project presentations!

Each project team will present a short summary. We'll go in reverse-alphabetical order.


<!--
 
 <h3>Learning as inference. Occam's razor.</h3>
 <p>Readings:
 <ul>
 <li> ProbMods wiki: <a href="http://projects.csail.mit.edu/church/wiki/Learning_as_Conditional_Inference">Learning as Conditional Inference</a>.
 <li> ProbMods wiki: <a href="http://projects.csail.mit.edu/church/wiki/Occam%27s_Razor">Occam's Razor</a>.
 <li> <a href="http://web.mit.edu/cocosci/Papers/cogsci00_FINAL.pdf">Word learning as Bayesian inference.</a> Tenenbaum and Xu (2000).
 <li> <a href="http://web.mit.edu/cocosci/Papers/structure-strength-reprint.pdf">Structure and strength in causal induction.</a> Griffiths and Tenenbaum (2005).
 <li>Optional: <a href="http://web.mit.edu/cocosci/Papers/bayes.pdf">Bayesian modeling of human concept learning.</a> Tenenbaum (1999).
 <li> Optional: <a href="http://web.mit.edu/cocosci/Papers/f881-XuTenenbaum.pdf"> Word learning as Bayesian inference: Evidence from preschoolers.</a> Xu and Tenenbaum (2005).
 <li> Optional: <a href="http://web.mit.edu/cocosci/Papers/nips99preprint.pdf">Rules and similarity in concept learning.</a> Tenenbaum (2000).
 </ul>
 </p>
 
 <h3>Heirarchical and mixture models.</h3>
 <p>(Work on <a href="projects.html">project proposals</a>!)</p>
 <p>Readings:
 <ul>
 <li> ProbMods wiki: <a href="http://projects.csail.mit.edu/church/wiki/Hierarchical_Models"> Hierarchical Models</a>
 <li> ProbMods wiki: <a href="http://projects.csail.mit.edu/church/wiki/Mixture_Models"> Mixture Models</a>
 <li> ProbMods book: <a href="restrictedpapers/ch9-main.pdf">Hierarchical Bayes</a> (<a href="restrictedpapers/probmodscomments.html">feedback</a>)
 <li> <a href="http://www.psy.cmu.edu/~ckemp/papers/KempPT06.pdf">Learning overhypotheses.</a> Kemp, Perfors, and Tenenbaum (2006).
 <li> Optional: <a>Object name learning provides on-the-job training for attention.</a> Smith, Jones, Landau, Gershko-Stowe, and Samuelson (2002).
 </ul>
 </p>
 
 
 
 
 <h3>Non-parametrics and relational models.</h3>
 <p><a href="projects.html">Project proposals</a> due Saturday!</p>
 <p>Readings:
 <ul>
 <li> ProbMods wiki: <a href="http://projects.csail.mit.edu/church/wiki/Non-Parametric_Models">Non-Parametric Models</a>
 <li> ProbMods book: <a href="restrictedpapers/ch8-main.pdf">Infinite Models</a> (<a href="restrictedpapers/probmodscomments.html">feedback</a>)
 <li> ProbMods book: <a>Relational Models</a> (<a href="restrictedpapers/probmodscomments.html">feedback</a>) 
 <li> <a href="http://www.psy.cmu.edu/~ckemp/papers/KempTGYU06.pdf">Learning systems of concepts with an infinite relational model.</a> Kemp, C., Tenenbaum, J. B., Griffiths, T. L., Yamada, T., Ueda, N. (2006).
 <li> <a href="http://stanford.edu/~ngoodman/papers/LTBC_psychreview_final.pdf">Learning a theory of causality.</a> Goodman, Ullman, and Tenenbaum (2011).
 <li> Optional: <a href="http://www.psy.cmu.edu/~ckemp/papers/kempgt10_learningtolearncausalmodels.pdf">Learning to learn causal models.</a> Kemp, C., Goodman, N., Tenenbaum, J. (2010).
 <li> Optional: <a href="http://books.nips.cc/papers/files/nips23/NIPS2010_1259.pdf"> Infinite Relational Modeling of Functional
 Connectivity in Resting State fMRI.</a> Morup, M. and Madsen, K.H. and Dogonowski, A.M. and Siebner, H. and Hansen, L.K. (2010).
 </ul>
 </p>
 </div>
 
 
 
 <h3>Logic, recursion, and grammar-based induction.</h3>
 <p>Readings:
 <ul>
 <li> ProbMods wiki: <a href="http://projects.csail.mit.edu/church/wiki/Recursive_Models">Recursive Models</a>
 <li> ProbMods book: <a href="restrictedpapers/ch11-main.pdf">Logical representations</a> (<a href="restrictedpapers/probmodscomments.html">feedback</a>)
 <li> ProbMods book: <a>Grammars for cognition</a> (<a href="restrictedpapers/probmodscomments.html">feedback</a>)
 <li> <a href="http://stanford.edu/~ngoodman/papers/RRfinal3.pdf">A rational analysis of rule-based concept learning.</a> Goodman, Tenenbaum, Feldman, and Griffiths (2008).
 <li> Optional: <a href="http://www.psy.cmu.edu/~ckemp/papers/kempr_kinshipcategoriesacrosslanguagesreflectgeneralcommunicativeprinciples.pdf">
 Kinship categories across languages reflect general communicative principles</a>. Kemp and Regier (2012).
 <li> Optional: <a href="http://stanford.edu/~ngoodman/papers/LTBC_psychreview_final.pdf">Learning a theory of causality.</a> Goodman, Ullman, and Tenenbaum (2011).
 <li> Optional: <a href="http://www.mit.edu/~ast/papers/structured-generative-concepts-cogsci2010.pdf">Learning Structured Generative Concepts.</a> Stuhlmueller, Tenenbaum, and Goodman (2010).
 <li> Optional: <a href="http://www.dectech.org/publications/LinksNick/Language/Language.pdf">Probabilistic models of language processing and acquisition.</a> Chater and Manning (2006).
 </ul>
 </p>
 </div>
 
 -->



# Course Projects

Your final project is an opportunity to get in-depth experience
applying the techniques we've discussed in class to a question that
interests you. In choosing a project, you should draw on your own
background, interests and strengths. You do not have to work on a
project that relates directly to the topics covered in the classes and readings: other topics
that pursue the general idea of probabilistic models of cognition are fine, and you should try
to work on a project that captures your interests within that fairly broad scope. Working
on existing research projects is okay, if you bring the techniques and ideas of
the class to bear.

You are encouraged (but not required) to do projects in small groups of two or three people.

Projects will generally contain both a probabilistic model of some aspect of human cognition and a behavioral expriment testing the model. Some ways you can go:

* Directly replicate the experiment and model in an existing paper. This is the most concrete way to go if you are new to both experiments and models. 
* Replicate an existing experiment (or possibly use existing data) that has not been modeled and consider different probabilistic models for the data.
* Extend the experiment and model in an existing paper in a new direction.
* Something brand new: choose an interesting phenomenon in human cognition; do an experiment and model it!

In all cases, you are encouraged to consider multiple models (for example, several variants of your theory) and pay careful attention to data analysis (for example, by doing bayesian model selection).

With approval of the instructor, a project could focus on AI rather than human behavior: use an idea we've discussed in class to implement an interesting new AI system. Similarly projects could focus on inference and infrastructure in PPLs by building a better algorithm, implementing a useful automatic analysis of programs, etc.


## Project proposal

Your proposal should be no more than one page long (single
spaced). Make sure that you cover the background, key question, and
methods of your project. The *background* should include the topic and
the context of your project, including other research in this area. 
The specific *question* you are planning to ask through your
project should be clearly stated. You should briefly describe the
*methods* you plan to use (your experimental
design, your modeling approach, your data analysis, and so on).

Email your proposal to the instructor as a PDF file by midnight on 5/1/15.


## Project update

Two weeks before your project presentation you should turn in a preliminary version of your paper. This should be a complete outline for all sections. It should have a full draft of your *introduction* and *background and related work* sections. In addition, it should have preliminary results from your modeling and/or experiments. Alltogether, these will probably take about 3 pages.

Email your preliminary report to the instructor as a PDF file by midnight on 5/15/15.


## Project presentation

Each person or team will have 10 minutes to present their project. We will go in
reverse-alphabetical order. The presentations should describe your question,
methods, and results at a high level.


## Project writeup

Your final project should be described in the format of a conference
paper, following the guidelines of paper submissions to the
Cognitive Science Society conference: [see the section "Submission formats" on this page](http://cognitivesciencesociety.org/conference2015/submissions.html). 
In particular, your paper should be no more than six pages long.
Your paper should cover the background behind your
project, the questions you are asking, your methods and results, and
your interpretation of these results.

Email your paper to the instructor as a PDF file by midnight on 6/5/15.


